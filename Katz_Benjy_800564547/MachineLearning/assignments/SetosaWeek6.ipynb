{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNVV3Om0ZpzfT6GHBmlZu1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","Code from here was adapted from: https://www.marktechpost.com/2021/04/08/logistic-regression-with-keras/\n","\n","https://keras.io/api/optimizers/\n","\n","https://www.tensorflow.org/guide/keras/train_and_evaluate\n","\n","Hands-on Machine Learning: Aurelien Geron\n","\n","Benjamin Katz"],"metadata":{"id":"35TDmQfMv9yv"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"WYuA1zVSv4ic","executionInfo":{"status":"ok","timestamp":1677528738031,"user_tz":300,"elapsed":139,"user":{"displayName":"Benjamin Katz","userId":"05452657003282238466"}}},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import keras"]},{"cell_type":"code","source":["import sklearn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from tensorflow.keras import backend as K\n","iris = datasets.load_iris()\n","X = np.array(iris[\"data\"][:, 2:])# petal length, petal width \n","y = np.array((iris[\"target\"] == 0).astype(int)) # 1 if Iris-Virginica, else 0\n"],"metadata":{"id":"oL0ZlcrywBZC","executionInfo":{"status":"ok","timestamp":1677528739168,"user_tz":300,"elapsed":3,"user":{"displayName":"Benjamin Katz","userId":"05452657003282238466"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#Seperate out testing data\n","#get a random distribution of indexes of the data\n","#this will allow testing on a random sample as opposed to just the begining of\n","#the data set or the end\n","shuffled_indexes = np.random.permutation(len(X))\n","#seperate out 20 percent for testing\n","test_size = int(len(X)*.2)\n","#get the testing and training indexes\n","test_indexes = shuffled_indexes[:test_size]\n","train_indexes = shuffled_indexes[test_size:]\n","#set the sets to the actual values\n","X_train = np.zeros((len(X)-test_size, 2))\n","y_train = np.zeros(len(X)-test_size)\n","X_test = np.zeros((test_size,2))\n","y_test = np.zeros(test_size)\n","\n","for i in range(len(train_indexes)):\n","  X_train[i] = X[train_indexes[i]]\n","  y_train[i] = y[train_indexes[i]]\n","for i in range(len(test_indexes)):\n","  X_test[i] = X[test_indexes[i]]\n","  y_test[i] = y[test_indexes[i]]\n"],"metadata":{"id":"Zhx7hfHewUrh","executionInfo":{"status":"ok","timestamp":1677528740629,"user_tz":300,"elapsed":197,"user":{"displayName":"Benjamin Katz","userId":"05452657003282238466"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["The following method of finding the F1 score was inspired by \n","https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras"],"metadata":{"id":"MiTigsb5gif6"}},{"cell_type":"code","source":["#F1 score is 2 times the recall times the precision over precision plus recall\n","#for further discusion see part one\n","def custom_f1(y_true, y_pred):\n","    #Recall is the true positives over all the actuall positives\n","    def recall_m(y_true, y_pred):\n","        #True positives are the positives it correctly predicted\n","        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        #epsilon is added here to assist in the division and ensure we do not divide by 0\n","        recall = TP / (Positives+K.epsilon())\n","        return recall\n","\n","  #precision is the true positives over the predicted positives, see part one for further discusion\n","    def precision_m(y_true, y_pred):\n","        #True positives are the positives it correctly predicted\n","        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","\n","        precision = TP / (Pred_Positives+K.epsilon())\n","        return precision\n","\n","    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n","\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"metadata":{"id":"QvenKlRUe7GQ","executionInfo":{"status":"ok","timestamp":1677529243162,"user_tz":300,"elapsed":255,"user":{"displayName":"Benjamin Katz","userId":"05452657003282238466"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["number_of_classes = 1\n","number_of_features = 2\n","model = Sequential()\n","#Tell the model we want to use the sigmoid function to map our data, and tell it that we are using two features\n","#The l2 regulaizer multiplies the square of the loss by 0.01 to add to the loss. Adding this regulizer makes the model converge slower\n","#and not stay entirely accurate when accuracy reaches 100 percent\n","#this makes sense since the goal of regularization is to avoid overfitting, we are not satisfied with perfect accuracy\n","model.add(Dense(number_of_classes,activation = 'sigmoid',input_dim = number_of_features,kernel_regularizer='l2' ))\n","#Optimize for a learning rate of 5 which was decided using trial and error\n","opt = keras.optimizers.Adam(learning_rate=5)\n","#Set gradient desent and find the linear regression with a given loss function of binary cross entrapy\n","model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[custom_f1])#, metrics = ['accuracy']\n","#actually perform the gradient desent over 40 epochs on the training data\n","model.fit(X_train, y_train, epochs=40)\n","#test the test data \n","print(\"Test loss, test accuracy: \" +str(model.evaluate(X_test,y_test)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeL-FlZFwDS9","executionInfo":{"status":"ok","timestamp":1677528785651,"user_tz":300,"elapsed":2250,"user":{"displayName":"Benjamin Katz","userId":"05452657003282238466"}},"outputId":"d81d226e-5450-45dd-fa95-d772ea6bb992"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n","4/4 [==============================] - 1s 4ms/step - loss: 5.7219 - custom_f1: 0.1098\n","Epoch 2/40\n","4/4 [==============================] - 0s 4ms/step - loss: 5.9877 - custom_f1: 0.0000e+00\n","Epoch 3/40\n","4/4 [==============================] - 0s 5ms/step - loss: 1.4148 - custom_f1: 0.6976\n","Epoch 4/40\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5329 - custom_f1: 0.9278\n","Epoch 5/40\n","4/4 [==============================] - 0s 5ms/step - loss: 1.1898 - custom_f1: 0.9853\n","Epoch 6/40\n","4/4 [==============================] - 0s 4ms/step - loss: 1.2979 - custom_f1: 0.9688\n","Epoch 7/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8609 - custom_f1: 1.0000\n","Epoch 8/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5113 - custom_f1: 0.9900\n","Epoch 9/40\n","4/4 [==============================] - 0s 5ms/step - loss: 0.6325 - custom_f1: 0.8681\n","Epoch 10/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4338 - custom_f1: 1.0000\n","Epoch 11/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4026 - custom_f1: 1.0000\n","Epoch 12/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.2646 - custom_f1: 0.9808\n","Epoch 13/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.2231 - custom_f1: 0.9549\n","Epoch 14/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.1655 - custom_f1: 0.9868\n","Epoch 15/40\n","4/4 [==============================] - 0s 5ms/step - loss: 0.1692 - custom_f1: 1.0000\n","Epoch 16/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.1117 - custom_f1: 0.9747\n","Epoch 17/40\n","4/4 [==============================] - 0s 6ms/step - loss: 0.0905 - custom_f1: 1.0000\n","Epoch 18/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0973 - custom_f1: 1.0000\n","Epoch 19/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0920 - custom_f1: 1.0000\n","Epoch 20/40\n","4/4 [==============================] - 0s 5ms/step - loss: 0.0881 - custom_f1: 1.0000\n","Epoch 21/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0870 - custom_f1: 1.0000\n","Epoch 22/40\n","4/4 [==============================] - 0s 5ms/step - loss: 0.0874 - custom_f1: 1.0000\n","Epoch 23/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0823 - custom_f1: 1.0000\n","Epoch 24/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0853 - custom_f1: 1.0000\n","Epoch 25/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0884 - custom_f1: 1.0000\n","Epoch 26/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0870 - custom_f1: 1.0000\n","Epoch 27/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0845 - custom_f1: 1.0000\n","Epoch 28/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0849 - custom_f1: 1.0000\n","Epoch 29/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0845 - custom_f1: 1.0000\n","Epoch 30/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0842 - custom_f1: 1.0000\n","Epoch 31/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0847 - custom_f1: 1.0000\n","Epoch 32/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0844 - custom_f1: 1.0000\n","Epoch 33/40\n","4/4 [==============================] - 0s 5ms/step - loss: 0.0844 - custom_f1: 1.0000\n","Epoch 34/40\n","4/4 [==============================] - 0s 5ms/step - loss: 0.0876 - custom_f1: 1.0000\n","Epoch 35/40\n","4/4 [==============================] - 0s 5ms/step - loss: 0.0878 - custom_f1: 1.0000\n","Epoch 36/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0864 - custom_f1: 1.0000\n","Epoch 37/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0859 - custom_f1: 1.0000\n","Epoch 38/40\n","4/4 [==============================] - 0s 5ms/step - loss: 0.0847 - custom_f1: 1.0000\n","Epoch 39/40\n","4/4 [==============================] - 0s 4ms/step - loss: 0.0832 - custom_f1: 1.0000\n","Epoch 40/40\n","4/4 [==============================] - 0s 7ms/step - loss: 0.0851 - custom_f1: 1.0000\n","1/1 [==============================] - 0s 238ms/step - loss: 0.0808 - custom_f1: 1.0000\n","Test loss, test accuracy: [0.0808037742972374, 1.0]\n"]}]}]}